

# M√≥dulo 2. Cadenas y Memoria


## Introducci√≥n al m√≥dulo

Bienvenidos al segundo m√≥dulo del curso de construcci√≥n de aplicaciones asistidas por LLM. En el primer m√≥dulo aprendiste a confeccionar instrucciones reutilizables para LLM, los prompt templates, y exploraste c√≥mo acoplar estas instrucciones en cadenas con especificadores de formato llamados output parsers. Sin embargo, estas cadenas de ejecuci√≥n eran cadenas de un solo turno de interacci√≥n entre la IA y el usuario humano. En este m√≥dulo aprender√°s a darle memoria y contexto a tus cadenas de ejecuci√≥n. Profundizaremos a√∫n m√°s en el funcionamiento de las cadenas y como limitar su memoria en el contexto del LCEL. Finalmente, pondr√°s a prueba tu conocimiento creando un chatbot que asiste las labores de un m√©dico realizando tareas secuenciales y que tiene como contexto en su memoria los datos espec√≠ficos de un paciente.

¬°Comencemos!


## Resultados de aprendizaje
Al finalizar esta m√≥dulo, estar√°s en capacidad de:

- Usar cadenas dotadas de memoria usando el LECL.
- Limitar el tama√±o del contexto cargado en la memoria de tus cadenas.


## Cronograma de actividades - M√≥dulo 2
| Actividad de aprendizaje       | Evidencia de aprendizaje | Semana       | Ponderaci√≥n |
|--------------------------------|---------------------------|--------------|--------------|
| EA1:  Cadenas y memoria         | EA1: Cadenas y memoria| Semana 4 y 5 | 25%         |
| **Total**                      |                           |              | **25 %**     |


<!--
Desarrollo tem√°tico
-->

## Cadenas

Hay varias maneras de instanciar cadenas de ejecuci√≥n en LangChain, algunas de las cuales fueron exploradas en el m√≥dulo 1. Recientemente, LangChain introdujo LangChain Expression Language (LCEL) como el est√°ndar para construir cadenas. Revisemos m√°s detalladamente de qu√© se trata:
!!! warning "Para tener en cuenta"
    Recuerda cargar tu llave en las variables de sistema si no lo has hecho:
    ```python
    import os
    import openai
    from dotenv import load_dotenv, find_dotenv

    # Cargar el archivo .env local
    _ = load_dotenv(find_dotenv()) 
    openai.api_key = os.environ['OPENAI_API_KEY']
    ```


### LangChain Expression Language (LCEL)

LangChain Expression Language (LCEL) es una sintaxis para definir cadenas. Permite componer objetos ejecutables (runnables)‚Äîobjetos que pueden ser ejecutados o encadenados‚Äîusando el operador **pipe** (|). Un runnable es cualquier componente que implementa la interfaz Runnable, lo que significa que puede procesar entradas y producir salidas. Ejemplos incluyen:

- Plantillas de prompt (ChatPromptTemplate), como lo hicimos en el m√≥dulo 1.

El operador de tuber√≠a (|) conecta estos componentes, pasando la salida de un Runnable como la entrada al siguiente. Por ejemplo:

```python
chain = prompt | llm | output_parser
```

Veamos un ejemplo en detalle:

=== "C√≥digo"
    ```python
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI
    from langchain_core.output_parsers import StrOutputParser

    # Define the prompt template
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a world-class technical documentation writer."),
        ("user", "{input}")
    ])

    # Initialize the LLM
    llm = ChatOpenAI(model="gpt-4", temperature=0.7)

    # Create the output parser
    output_parser = StrOutputParser()

    # Compose the chain
    chain = prompt | llm | output_parser

    # Invoke the chain
    response = chain.invoke({"input": "How can LangSmith help with testing?"})
    print(response)
    ```

=== "Salida"
    ```bash
    LangSmith provides a suite of tools designed to assist with software testing, particularly in the realm of language translation and localization. It can be invaluable for teams developing software that needs to function in multiple languages and regions.

    1. **Automated Testing:** LangSmith can automatically test your software‚Äôs language functionality, ensuring translations are accurate, context-appropriate, and that all elements of the user interface are correctly localized.

    2. **Quality Assurance:** By checking translations against a comprehensive database, LangSmith aids in maintaining high translation quality, reducing the risk of miscommunication or confusion for users.

    3. **Regression Testing:** When updates or changes are made to the software, LangSmith can help ensure that these changes have not negatively affected the language functionality.

    4. **Cultural Accuracy:** Beyond simple language translation, LangSmith can also verify that cultural nuances and local customs are appropriately considered, which can greatly improve user experience.

    5. **Reporting and Analytics:** LangSmith provides detailed reports on testing outcomes, highlighting any issues or potential areas for improvement. This can provide valuable insights for your development team.

    By integrating LangSmith into your testing process, you can more effectively ensure the quality and accuracy of your software's multilingual and multicultural features, improving overall user experience and satisfaction.
    ```

### Funciones (envueltas con RunnableLambda)

Podemos usar cadenas con funciones al comienzo de la l√≠nea de ejecuci√≥n, un cierto tipo de funci√≥n muy vers√°til son las funciones de la clase RunnableLambda. Estas est√°n dise√±adas para integrar funciones personalizadas de Python en cadenas de LangChain. Permiten a los desarrolladores envolver funciones de Python arbitrarias (o funciones lambda) en un objeto ejecutable, haci√©ndolas compatibles con la sintaxis del LCEL. Es decir, convierte una funci√≥n de Python en un componente encadenable.

La funci√≥n debe aceptar una entrada compatible con la salida del paso anterior y producir una salida compatible con el siguiente paso.

Por ejemplo:

```python
from langchain_core.runnables import RunnableLambda

# Wrap a function
runnable_sumaUno = RunnableLambda(lambda x: x + 1)  # suma 1 a la entrada
runnable_cuadrado = RunnableLambda(lambda x: x**2)  # eleva al cuadrado la entrada

# La cadena de ejecuci√≥n ser√≠a
chain = runnable_sumaUno | runnable_cuadrado  # encadena las funciones
result = chain.invoke(5)  # (5 + 1) ** 2 = 36
print(result)  # Output: 36
```
Vemos otro ejemplo:

```python
from langchain_core.runnables import RunnableLambda

# Define funciones lambda individuales
add_prefix = RunnableLambda(lambda x: f"Hello, {x}!")
to_upper = RunnableLambda(lambda x: x.upper())

# Construir la cadena
chain = add_prefix | to_upper

# Invocar la cadena
result = chain.invoke("Alice")
print(result)  # Salida: HELLO, ALICE!
```

Veamos c√≥mo construir una cadena que toma una consulta de usuario, la formatea en un prompt, la procesa con un LLM y extrae la primera palabra de la respuesta.

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI  # o usar otro LLM

# Definir componentes

# Crear un prompt template y llenarlo
format_prompt = RunnableLambda(
    lambda x: PromptTemplate.from_template("Answer briefly: {query}").format(query=x)
)

# Extraer la primera palabra de la respuesta
extract_first_word = RunnableLambda(
    lambda x: x.content.split()[0] if hasattr(x, 'content') else x.split()[0]
)

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# Construir la cadena
chain = format_prompt | llm | extract_first_word

# Invocar la cadena
result = chain.invoke("¬øC√≥mo se llama el presidente de Colombia?")
print(result)  # Salida: Iv√°n (recuerda que gpt-3.5 fue entrenado en datos hasta octubre de 2023)
```

La cadena procesa la consulta de entrada paso a paso: formatea la consulta, la procesa con el LLM y extrae la primera palabra de la respuesta. Puedes reemplazar `ChatOpenAI` con otro modelo (por ejemplo, `HuggingFaceHub`) si es necesario.
Language models (ChatOpenAI).

## Memoria

La memoria es el mecanismo por el cual le damos al LLM contexto de nuestras interacciones previas.


Para explorar el uso de la memoria, instanciaremos una cadena de conversaci√≥n a partir de la clase preconstruida `ConversationChain`. Importamos los m√≥dulos necesarios:

```python
from langchain.chains.conversation.base import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
```

Usaremos GPT-4 y el modelo de chat de OpenAI:

```python
chat_model = ChatOpenAI(model_name="gpt-4", temperature=0, streaming=True)
```

Para instanciar una cadena de conversaci√≥n de la clase `ConversationChain`, necesitamos instanciar primero el objeto de clase `Memory` que nos servir√° para administrar la memoria de las interacciones en la conversaci√≥n. Lo hacemos de la siguiente manera:

```python
memory = ConversationBufferMemory() # Es una instancia de la clase que contiene los controles de la memoria
```

As√≠, la cadena de conversaci√≥n la instanciaremos como:

```python
chain = ConversationChain(llm=chat_model, memory=memory, verbose=True)
```
Ahora est√° todo listo para que comencemos nuestra conversaci√≥n:

=== "C√≥digo"
    ```python
    chain.invoke("Hola, mi nombre es Juan, ¬øc√≥mo est√°s?")
    ```

=== "Salida"
    ```bash
    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:

    Human: Hola, mi nombre es Juan, ¬øc√≥mo est√°s?
    AI:

    > Finished chain.

        {'input': 'Hola, mi nombre es Juan, ¬øc√≥mo est√°s?',
    'history': '',
    'response': '¬°Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as√≠ que no tengo emociones como los humanos, pero estoy aqu√≠ para ayudarte y conversar contigo. ¬øEn qu√© puedo asistirte hoy?'}
    ```
Si pregunto aluna cosa adicional, por ejeplo cuando es 2 + 2. No olvidar√° mi nombre:

=== "C√≥digo"
    ```python
    chain.invoke("cuanto es 2 + 2?")
    chain.invoke("cual es mi nombre?")
    ```

=== "Salida"
    ```bash
    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:
    Human: Hola, mi nombre es Juan, ¬øc√≥mo est√°s?
    AI: ¬°Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as√≠ que no tengo emociones como los humanos, pero estoy aqu√≠ para ayudarte y conversar contigo. ¬øEn qu√© puedo asistirte hoy?
    Human: cuanto es 2 + 2?
    AI: 2 + 2 es igual a 4. Es una de las operaciones matem√°ticas m√°s b√°sicas y es un buen ejemplo de c√≥mo funcionan las sumas. Si tienes m√°s preguntas de matem√°ticas o cualquier otro tema, estar√© encantado de ayudarte.
    Human: ¬øCu√°l es mi nombre?
    AI:

    {'input': '¬øCu√°l es mi nombre?',
    'history': 'Human: Hola, mi nombre es Juan, ¬øc√≥mo est√°s?\nAI: ¬°Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial, as√≠ que no tengo emociones como los humanos, pero estoy aqu√≠ para ayudarte y conversar contigo. ¬øEn qu√© puedo asistirte hoy?\nHuman: cuanto es 2 + 2?\nAI: 2 + 2 es igual a 4. Es una de las operaciones matem√°ticas m√°s b√°sicas y es un buen ejemplo de c√≥mo funcionan las sumas. Si tienes m√°s preguntas de matem√°ticas o cualquier otro tema, estar√© encantado de ayudarte.',
    'response': 'Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversaci√≥n. Si tienes m√°s preguntas o necesitas ayuda con algo m√°s, no dudes en dec√≠rmelo.'}
    ```
La IA responde que *Tu nombre es Juan. Me lo dijiste al comienzo de nuestra conversaci√≥n*. Lo cual no era posible en el m√≥dulo 1 cuando invoc√°bamos el modelo sin memoria.

## La ventana de contexto

La memoria es un recurso costoso, pues los modelos tienen una capacidad limitada para guardar el contexto de las conversaciones. Esta caracter√≠stica es llamada **la ventana de contexto**. A medida que los modelos se han vuelto m√°s avanzados, las ventanas de contexto ofrecidas son cada vez m√°s grandes. 

- GPT-3: 4096 tokens
- GPT-3.5-turbo: 4096 tokens
- GPT-4: 8192 tokens (con una versi√≥n extendida de 32768 tokens)

Sin embargo, debemos tener en cuenta que el modelo siempre tratar√° de relacionar sus respuestas con el contexto de la conversaci√≥n. Esto puede ser beneficioso, pero al mismo tiempo, puede introducir ruido a la conversaci√≥n, y el modelo puede generar respuestas menos precisas si est√° distra√≠do en un contexto muy amplio. Por este motivo, resulta conveniente tener la posibilidad de limitar la memoria que queremos dar a las aplicaciones asistidas por IA. Veremos ahora algunos tipos de memoria y sus usos.

## Tipos de Memoria en LangChain

LangChain ofrece varios tipos de memoria que se pueden utilizar para gestionar el contexto en las aplicaciones de inteligencia artificial

crearemos una pequena encapsulaci√≥n para ilistrar el uso de las dos clases principales:

```python
class ChatBot:
    def __init__(self, memory):
        self.chat_model  =  ChatOpenAI(model_name= "gpt-4o", temperature= 0, streaming= True)
        self.memory = memory
        self.chain = ConversationChain(llm=self.chat_model, memory=self.memory, verbose=True)
        
# metodo para responder
    def pregunta(self, pregunta: str):        
        response = self.chain.predict(input = pregunta)
        print(response)
        return 0
```
## Tipos de Memoria en LangChain

**ConversationBufferMemory**: 
   - **Uso**: Esta memoria almacena todo el historial de la conversaci√≥n sin ning√∫n l√≠mite. Es √∫til cuando se desea mantener un registro completo de todas las interacciones previas, como lo hicimos en el ejemplo anterior.

   Por ejemplo, un chat con memoria ilimitada se crear√≠a de la siguiente manera:

   ```python
   from langchain.memory import ConversationBufferMemory

   mucha_memoria = ConversationBufferMemory()
   genioBot = ChatBot(mucha_memoria)
   ```
**ConversationBufferWindowMemory**:
   - **Uso**: Similar a `ConversationBufferMemory`, pero con un par√°metro llamado `window_size` que permite recordar solo un n√∫mero fijo de interacciones recientes.

   Creamos un bot muy inteligente pero con memoria limitada usando `ConversationBufferWindowMemory`:

=== "C√≥digo"
    ```python
    from langchain.memory import ConversationBufferWindowMemory

    # Memoria con ventana de 1
    poca_memoria = ConversationBufferWindowMemory(k=1)

    # Instanciamos un bot con poca memoria
    olvidoBot = ChatBot(poca_memoria)
    olvidoBot.pregunta("Hola, mi nombre es Juan, ¬øc√≥mo est√°s?")
    olvidoBot.pregunta("¬øCu√°nto es 2 + 5?")
    olvidoBot.pregunta("¬øCu√°l es mi nombre?") # No sabe el nombre
    ```

=== "Salida"
    ```bash hl_lines="9"
    > Entering new ChatBot session...
    Human: Hola, mi nombre es Juan, ¬øc√≥mo est√°s?
    AI: ¬°Hola, Juan! Estoy muy bien, gracias por preguntar. Soy una inteligencia artificial dise√±ada para ayudarte con informaci√≥n y responder a tus preguntas. ¬øEn qu√© puedo asistirte hoy?

    Human: ¬øCu√°nto es 2 + 5?
    AI: 2 + 5 es igual a 7. Si tienes m√°s preguntas de matem√°ticas o cualquier otra cosa en mente, ¬°estar√© encantado de ayudarte!

    Human: ¬øCu√°l es mi nombre?
    AI: Lo siento, no tengo la capacidad de saber tu nombre a menos que me lo hayas dicho antes en esta conversaci√≥n. Si quieres, puedes dec√≠rmelo ahora y lo recordar√© para el resto de nuestra charla.
    ```
`olvidoBot` solo recordar√° una interacci√≥n a la vez debido a `window_size=1`. Cuando preguntamos "¬øCu√°l es mi nombre?", ya no recuerda la interacci√≥n donde le dijimos nuestro nombre, por lo que no puede recordarlo.

`ConversationSummaryMemory` almacena un resumen de la conversaci√≥n, ideal para interacciones muy largas donde no es necesario retener todos los detalles. <!--  NOta terminar esta parte, poner un ejemplo aqu√≠-->

!!! warning "Para tener en cuenta"
    Las gu√≠as m√°s recientes recomiendan utilizar la funci√≥n `trim_messages`, que proporciona una forma flexible de gestionar el historial de la conversaci√≥n. Las funcionalidades aqu√≠ expuestas est√°n siendo migradas a la plataforma de LangGraph, por lo que est√°n fuera del alcance de este curso. Ver√°s el mensaje `LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/`.


# Entendiendo la Funci√≥n `trim_messages`

La funci√≥n `trim_messages` es una utilidad en LangChain dise√±ada para reducir el tama√±o de un historial de chat a un n√∫mero espec√≠fico de tokens o mensajes, asegurando que el historial recortado siga siendo v√°lido para los modelos de chat. Un historial de chat v√°lido t√≠picamente:

- **Comienza con:**
  - Un `HumanMessage`, o
  - Un `SystemMessage` seguido de un `HumanMessage`.

- **Termina con:**
  - Un `HumanMessage`, o
  - Un `ToolMessage` (com√∫n en conversaciones basadas en agentes).

 Al recortar mensajes m√°s antiguos o menos relevantes, `trim_messages` ayuda a enfocar el modelo en el contexto reciente y pertinente, evitanto informaci√≥n que que pueda distraer al modelo (no simepre un contexto grande es mejor).

La funci√≥n `trim_messages` opera tomando varios par√°metros para controlar c√≥mo se realiza el recorte:

| Par√°metro        | Descripci√≥n                                                                                                                                                   |
|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `messages`       | La secuencia de mensajes (por ejemplo, `HumanMessage`, `AIMessage`, `SystemMessage`) a recortar.                                                              |
| `max_tokens`     | El n√∫mero m√°ximo de tokens que deben tener los mensajes recortados.                                                                                           |
| `strategy`       | La estrategia de recorte: "first" (mantiene los primeros mensajes) o "last" (mantiene los m√°s recientes, a menudo preferido para conversaciones).              |
| `token_counter`  | Una funci√≥n o LLM utilizado para contar tokens en los mensajes, como el m√©todo de conteo de tokens incorporado de un LLM.                                      |
| `include_system` | Un booleano (por defecto: `False`) que especifica si se debe mantener el `SystemMessage` al principio si est√° presente.                                        |
| `allow_partial`  | Un booleano (por defecto: `False`) que permite dividir un mensaje si solo parte de √©l puede incluirse para cumplir con el l√≠mite de tokens.                    |

La funci√≥n devuelve una lista de mensajes recortados que se ajustan a los l√≠mites especificados mientras mantienen la coherencia del contexto.

Consideremos un ejemplo pr√°ctico donde tenemos un chatbot que utiliza una cadena para procesar consultas de usuario. El chatbot ha estado funcionando durante varias interacciones, y el historial de chat se ha alargado. Necesitamos recortar el historial para ajustarlo a un l√≠mite de 100 tokens, manteniendo los mensajes m√°s recientes y el `SystemMessage`.

Primero, definamos un historial de chat de ejemplo:

```python
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

chat_history = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="Hello, my name is Juan how are you?"),
    AIMessage(content="I'm doing well, thank you. How can I help you today?"),
    HumanMessage(content="Can you tell me about the weather today?"),
    AIMessage(content="Sure, let me check that for you. [checks weather] It's sunny with a high of 75 degrees."),
    HumanMessage(content="What's the capital of France?"),
    AIMessage(content="The capital of France is Paris."),
]
```

Ahora, usaremos `trim_messages` para recortar este historial a 100 tokens, manteniendo los mensajes m√°s recientes e incluyendo el `SystemMessage`:

```python
from langchain_core.messages.utils import trim_messages
from langchain_openai import OpenAI

# Inicializar un LLM para el conteo de tokens
llm = OpenAI(model="gpt-3.5-turbo-1106")

# Recortar los mensajes
trimmed_history = trim_messages(
    messages=chat_history,
    max_tokens=100,
    strategy="last",
    token_counter=llm.get_num_tokens_from_messages,
    include_system=True, # para mantener el SystemMessage en la memoria
)
```

Si el conteo total de tokens del historial original excede los 100, el historial recortado podr√≠a verse as√≠:

```python
[
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What's the capital of France?"),
    AIMessage(content="The capital of France is Paris."),
]
```

Esto mantiene la conversaci√≥n enfocada y dentro de la ventana de contexto especificada, pero no recordar√° nuestro nombre. A continuaci√≥n, se presenta el c√≥digo completo:

=== "C√≥digo"
    ```python
    from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
    from langchain_core.messages.utils import trim_messages
    from langchain_core.runnables import RunnableLambda
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    # Inicializar el LLM
    # Reemplazar 'your-openai-api-key' con tu clave API real de OpenAI
    llm = ChatOpenAI(model="gpt-3.5-turbo-1106")

    # Definir la plantilla de prompt
    # La plantilla incluye un mensaje de sistema, un marcador de posici√≥n para el historial de chat y la entrada del usuario
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("placeholder", "{chat_history}"),  # Importante: este es el marcador de posici√≥n para el historial de chat
        ("human", "{input}")
    ])

    # Definir una funci√≥n para recortar mensajes
    # Esta funci√≥n recorta el historial de chat para ajustarse a un l√≠mite de tokens especificado
    def trim_chat_history(messages):
        return trim_messages(
            messages=messages,
            max_tokens=100,  # Limitar a 100 tokens para ajustarse al contexto del modelo
            strategy="last",  # Conservar los mensajes m√°s recientes
            token_counter=llm.get_num_tokens_from_messages,  # Usar el contador de tokens del LLM
            include_system=True  # Preservar el mensaje del sistema
        )
    ```

=== "Salida"
    ```bash
    LLM Response: You haven't provided your name. Can you please share it with me?
    ```

!!! tip "üìñ Para aprender m√°s"
    El `("placeholder", "{chat_history}")` en el `ChatPromptTemplate` es un componente clave del sistema de plantillas de prompt de LangChain, utilizado para definir un espacio en el prompt donde se insertar√° una secuencia de mensajes (por ejemplo, el historial de la conversaci√≥n).

    === "Detalles"
        message type and its content or template.

        Tipos comunes de mensajes incluyen:
        - `("system", "...")`: Un mensaje del sistema que define el rol o las instrucciones del asistente.
        - `("human", "...")`: Un mensaje de entrada del usuario.
        - `("ai", "...")`: Un mensaje de respuesta del asistente.
        - `("placeholder", "{variable_name}")`: Un marcador de posici√≥n para una secuencia de mensajes o datos que se proporcionar√°n m√°s tarde.

        La tupla `("placeholder", "{chat_history}")` indica que la variable llamada `chat_history` contendr√° una lista de mensajes (por ejemplo, `SystemMessage`, `HumanMessage`, `AIMessage`) que se insertar√°n en esa posici√≥n en el prompt.
Esta cadena procesa una lista √∫nica de mensajes como contexto, pero no es propiamente una memoria de la conversaci√≥n, ya que es fija. Idealmente, queremos que la memoria sea persistente, es decir, que recuerde un cierto n√∫mero de interacciones. Para este prop√≥sito, podemos integrar `RunnableWithMessageHistory` a nuestra cadena para almacenar el historial de la conversaci√≥n.

## A√±adiendo Memoria Persistente para Chatbots

En el ejemplo anterior, la cadena procesa una lista plana de mensajes sin mantener el estado entre invocaciones:

```python
chain = (
    RunnableLambda(trim_chat_history)  # Recorta los mensajes directamente
    | prompt  # Formatea los mensajes recortados en el prompt
    | llm  # Genera una respuesta usando el LLM
)
```

Para a√±adir memoria persistente, envolveremos esta cadena con `RunnableWithMessageHistory`. Al implementar `RunnableWithMessageHistory` en LCEL, necesitamos definir una pipeline de base, y esta a su vez se define a partir de una plantilla de prompt y un LLM. Comencemos definiendo un nuevo prompt template; en este caso, usaremos `from_message`, para lo cual importaremos los m√≥dulos:

```python
from langchain.prompts import (
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    ChatPromptTemplate
)
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
```

Dividiremos el proceso en los siguientes pasos:

1. **Crear la pipeline base.** Como lo hemos hecho antes, definimos el prompt y un LLM. Por ejemplo:

    ```python
    # Definir el prompt del sistema al estilo de Don Quijote
    system_prompt = "Eres un asistente √∫til que responde en una sola oraci√≥n en espa√±ol, al estilo de Don Quijote de la Mancha."

    # Crear la plantilla de prompt para el chat
    prompt_template = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_prompt),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{query}"),
    ])

    # Inicializar el LLM
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

    # Crear la pipeline base
    pipeline = prompt_template | llm
    ```

    Esta es nuestra pipeline base sobre la cual a√±adiremos memoria usando `RunnableWithMessageHistory` y de esta forma retener el historial de conversaciones. Necesitaremos configurar un almac√©n de historial de chat, puede ser un simple diccionario:

2. **Configurar la Gesti√≥n del Historial de Chat.** Para a√±adir memoria, necesitamos un mecanismo para almacenar y recuperar el historial de conversaciones para diferentes sesiones de usuario. Nuestro `RunnableWithMessageHistory` requiere que nuestra pipeline est√© envuelta en un objeto `RunnableWithMessageHistory`. Este objeto necesita algunos par√°metros de entrada. Uno de ellos es `get_session_history`, que requiere una funci√≥n que devuelva un objeto `ChatMessageHistory` basado en un ID de sesi√≥n. Definimos esta funci√≥n nosotros mismos:

    ```python
    # Definir el historial de chat
    chat_history = [
        HumanMessage(content="Saludos, mi nombre es Juan, ¬øc√≥mo te hallas?"),
        AIMessage(content="En verdad, me hallo en buen estado, y estoy presto a servirte, ¬øcu√°l es tu deseo?"),
        HumanMessage(content="¬øPuedes hablarme del tiempo que hoy nos acompa√±a?"),
        AIMessage(content="Ciertamente, el d√≠a se muestra soleado con un calor apacible que alcanza los 24 grados."),
        HumanMessage(content="¬øCu√°l es la capital de Francia?"),
        AIMessage(content="La capital de Francia es Par√≠s, noble villa de gran renombre."),
    ]

    # Configurar la gesti√≥n del historial de chat con un historial predefinido
    chat_dict = {}
    session_id = "id_123"
    chat_dict[session_id] = InMemoryChatMessageHistory()
    chat_dict[session_id].add_messages(chat_history)

    def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:
        if session_id not in chat_dict:
            chat_dict[session_id] = InMemoryChatMessageHistory()
        return chat_dict[session_id]
    ```

    `chat_dict` es un diccionario que asigna IDs de sesi√≥n a objetos `InMemoryChatMessageHistory`, los cuales almacenan secuencias de mensajes (por ejemplo, mensajes del usuario y respuestas del asistente).

    `get_chat_history` es una funci√≥n que:
    - Toma un `session_id` (por ejemplo, "id_123") como entrada.
    - Verifica si existe un historial de chat para ese `session_id` en `chat_dict`.
    - Si no, crea un nuevo `InMemoryChatMessageHistory` y lo almacena en `chat_dict`.
    - Luego, devuelve el objeto de historial de chat correspondiente.

    !!! Tip "Para aprender m√°s"
        Esta configuraci√≥n asegura que cada sesi√≥n de usuario tenga su propio historial de chat aislado, permitiendo que m√∫ltiples usuarios interact√∫en simult√°neamente sin mezclar conversaciones. Sin embargo, para entornos de producci√≥n, el almacenamiento en memoria no es ideal debido a su volatilidad (los datos se pierden al reiniciar la aplicaci√≥n). Las alternativas incluyen:
        - `RedisChatMessageHistory` para almacenamiento escalable y persistente con Redis.
        - `PostgresChatMessageHistory` para almacenamiento respaldado por bases de datos.
        - `FileChatMessageHistory` para persistencia basada en archivos.

        Por ejemplo, para usar Redis, debes usar el paquete Redis (`%pip install --upgrade --quiet redis`), iniciar un servidor Redis (por ejemplo, a trav√©s de Docker) y definir un `get_message_history` con `RedisChatMessageHistory`, como se muestra en la documentaci√≥n de LangChain sobre Integraciones de Memoria.

3. **Envolver la Pipeline Base con `RunnableWithMessageHistory`.** Para a√±adir memoria, envolvemos la pipeline base con `RunnableWithMessageHistory`, que gestiona el historial de chat para el `Runnable`:

    ```python
    # Envolver la pipeline con RunnableWithMessageHistory
    pipeline_with_history = RunnableWithMessageHistory(
        pipeline,
        get_session_history=get_chat_history,
        input_messages_key="query",
        history_messages_key="history"
    )
    ```

    Aqu√≠, `pipeline` es el `Runnable` base que vamos a envolver (en este caso, `prompt_template | llm`). `get_session_history` es la funci√≥n (`get_chat_history`) que devuelve el historial de chat para un ID de sesi√≥n dado. Esta funci√≥n debe tomar un `session_id` y devolver una instancia de `BaseChatMessageHistory`. `input_messages_key` especifica la clave en el diccionario de entrada que contiene el mensaje actual del usuario. En el c√≥digo, es "query", lo que significa que la entrada espera algo como `{"query": "What is my name again?"}`. `history_messages_key` especifica la clave donde el historial de la conversaci√≥n debe ser inyectado en la entrada. En el c√≥digo, es "history", lo que significa que el historial (una lista de objetos `BaseMessage`) se a√±ade bajo esta variable de entrada.

    Al invocar, `RunnableWithMessageHistory` recupera el historial de chat para el ID de sesi√≥n usando `get_chat_history`. Aumenta el diccionario de entrada a√±adiendo el historial bajo la clave `history_messages_key` (por ejemplo, `{"query": "What is my name again?", "history": [...]}`). La entrada aumentada se pasa a la pipeline base, que incluye el historial en el prompt a trav√©s de `MessagesPlaceholder`. Despu√©s de que la pipeline genera una respuesta, el historial de chat se actualiza con el nuevo mensaje del usuario y la respuesta del asistente.

Y listo, nuestro historial de chat ahora se memorizar√° y recuperar√° cada vez que invoquemos nuestro runnable con el mismo ID de sesi√≥n.

=== "C√≥digo"
    ```python
    # Invocar la pipeline para demostrar la memoria
    result1 = pipeline_with_history.invoke(
        {"query": "¬øCu√°l es mi nombre?"},
        config={"session_id": "id_123"}
    )
    print(result1.content)  # Esperado: "Vuestro nombre, seg√∫n me hab√©is dicho, es Juan."

    result2 = pipeline_with_history.invoke(
        {"query": "¬øQu√© m√°s sabes de Francia?"},
        config={"session_id": "id_123"}
    )
    print(result2.content)
    ```

=== "Salida"
    ```bash
    Vuestro nombre es Juan, valeroso caballero que busca conocimiento y respuestas.
    Francia es tierra de exquisita gastronom√≠a, arte refinado y hermosos paisajes, dignos de ser explorados y admirados.
    ```

Nuestro chat ahora tiene la capacidad de recordar todas las interacciones. Vemos:

```python
result2 = pipeline_with_history.invoke(
    {"query": "El nombre de mi madre es Maria"},
    config={"session_id": "id_123"}
)
print(result2.content) 

result2 = pipeline_with_history.invoke(
    {"query": "¬øQui√©n es Maria en mi vida?"},
    config={"session_id": "id_123"}
)

chat = get_chat_history('id_123')

for msg in chat.messages:
    print(msg.content)
```

=== "Salida"
    ```bash
    Saludos, mi nombre es Juan, ¬øc√≥mo te hallas?
    En verdad, me hallo en buen estado, y estoy presto a servirte, ¬øcu√°l es tu deseo?
    ¬øPuedes hablarme del tiempo que hoy nos acompa√±a?
    Ciertamente, el d√≠a se muestra soleado con un calor apacible que alcanza los 24 grados.
    ¬øCu√°l es la capital de Francia?
    La capital de Francia es Par√≠s, noble villa de gran renombre.
    ¬øCu√°l es mi nombre?
    Vuestro nombre es Juan, valeroso caballero que busca conocimiento y respuestas.
    ¬øQu√© m√°s sabes de Francia?
    Francia es tierra de exquisita gastronom√≠a, arte refinado y hermosos paisajes, dignos de ser explorados y admirados.
    El nombre de mi madre es Maria
    Vuestra madre, Mar√≠a, posee un nombre tan puro y bello como el de la Virgen Santa.
    ¬øQui√©n es Maria?
    Mar√≠a es un nombre com√∫n entre las mujeres, pero tambi√©n es el nombre de la madre de Jes√∫s, la Virgen Mar√≠a, figura importante en la religi√≥n cat√≥lica.
    ¬øQui√©n es Maria en mi vida?
    Mar√≠a, en vuestra vida, es la mujer que os dio la vida, os cuid√≥ con amor y os gui√≥ en vuestro camino, como una luz en la oscuridad.
    ```

!!! tip "üìñ Para aprender m√°s"
    Hasta aqu√≠ hemos reproducido el comportamiento de la clase `ConversationBufferMemory` que describimos al principio. Sin embargo, esta clase ser√° deprecada en las versiones futuras de LangChain. Para ver c√≥mo crear *wrappers* de los dem√°s tipos de memoria, puedes consultar el siguiente material:
    **Art√≠culo**: Introducci√≥n a los Tipos de Memoria en LangChain  
    **URL**: [https://www.aurelio.ai/learn/langchain-memory-types](https://www.aurelio.ai/learn/langchain-memory-types)

¬°Felicidades por llegar al final del m√≥dulo 2! Has aprendido c√≥mo crear cadenas con memoria utilizando el LCEL. Ahora est√°s en capacidad de crear chatbots funcionales. Te invito a realizar la actividad de aprendizaje para que pongas en pr√°ctica lo aprendido.

 **Glosario**

- **Configurable runnables**: En LangChain, son funciones ejecutables que pueden personalizarse din√°micamente en tiempo de ejecuci√≥n utilizando un objeto `RunnableConfig`. Esto permite pasar par√°metros como el nombre de la ejecuci√≥n, etiquetas o metadatos para controlar el comportamiento, como los l√≠mites de concurrencia o recursi√≥n.

- **Context window**: La cantidad m√°xima de tokens de entrada (texto, datos, etc.) que un modelo de chat puede procesar en una sola interacci√≥n, determinada por la arquitectura del modelo.

- **langchain**: Un paquete de Python que proporciona componentes de alto nivel para construir aplicaciones con modelos de lenguaje, como cadenas preconstruidas y herramientas para tareas comunes.

- **langchain-community**: Una colecci√≥n de componentes e integraciones contribuidas por la comunidad para LangChain, que extiende su funcionalidad con herramientas de terceros.

- **langchain-core**: El paquete fundamental de LangChain, que contiene interfaces centrales, abstracciones base e implementaciones en memoria para construir cadenas y funciones ejecutables.

- **langgraph**: Una extensi√≥n de LangChain para orquestar flujos de trabajo y pipelines complejos, permitiendo una gesti√≥n avanzada de estado y procesos de m√∫ltiples pasos.

- **langserve**: Una herramienta para desplegar funciones ejecutables de LangChain como endpoints de API REST utilizando FastAPI. Principalmente soporta funciones ejecutables de LangChain, con compatibilidad limitada para LangGraph.

- **Managing chat history**: M√©todos y t√©cnicas para almacenar, recuperar y mantener el contexto conversacional a trav√©s de m√∫ltiples interacciones en una aplicaci√≥n basada en chat.

- **RunnableConfig**: Un objeto de configuraci√≥n en LangChain para pasar par√°metros de tiempo de ejecuci√≥n a funciones ejecutables, incluyendo `run_name`, `run_id`, etiquetas, metadatos, `max_concurrency`, `recursion_limit` y otras configuraciones personalizables.



## Evidencia de Aprendizaje                 


| **M√≥dulo 2** | **Cadenas y Memoria** |
|--------------|-------------------------------------------------------------|
| **EA1.**     |   **Generaci√≥n de Informes de Salud Utilizando Archivos CSV **|


En este proyecto practicar√°s el uso de cadenas para desrrollar un sistema que reliza tareas secuenciales y ramificadas, cargar√°s los resitado de estas operacones en el bufer de memoria de un chatbot.
## Instrucciones
Descarga el archivo [healthcare_report.csv](../assets/resources/healthcare_report.csv) proporcionado y, usando LECL, desarrolla un sistema capaz de:
1. **Procesar un informe de salud original en espa√±ol**: lee el informe desde el archivo CSV.
2. **Traducir el informe al ingl√©s**: usa una cadena para traducir el texto.
3. **Resumir el informe traducido**: genera un resumen breve en ingl√©s.
4. **Extraer indicadores clave de salud del resumen**: identifica elementos clave (e.g., s√≠ntomas, duraci√≥n).
5. **Generar un plan de tratamiento basado en los indicadores clave**: prop√≥n pasos de tratamiento.
6. **Detectar el idioma original del informe**: determina si el informe original est√° en espa√±ol.
7. **Generar una recomendaci√≥n de seguimiento en el idioma detectado**: devuelve una recomendaci√≥n en espa√±ol.

Finalmete carga el infome m√©dico del paciente en la memoria y crea un chat bot que est√© en capacidad de responder preguntas sobre el tratamiento indicado. Demuestra su uso con algunas llamas al chat


---

Guarda los documentos con la siguiente nomenclatura:

- **Apellido_Nombre del estudiante.ipynb**  
**Ejemplo:**  
- L√≥pez_Karla.ipynb

Finalmente, haz clic en el bot√≥n **Cargar Tarea**, sube tu archivo y presiona el bot√≥n **Enviar** para remitirlo a tu profesor con el fin de que lo eval√∫e y retroalimente. |

!!! tip "üìñ Nota"
    Conoce los criterios de evaluaci√≥n de esta evidencia de aprendizaje consultando la r√∫brica que encontrar√°s a continuaci√≥n.

| **Criterios**             | **Ponderaci√≥n** |                       |                       |                       |                       | **Totales** |
|---------------------------|------------------|-----------------------|-----------------------|-----------------------|-----------------------|------------|
|                           | **70**           | **50**                | **5**                 | **0**                 |                       |            |
| **Calidad de las Soluciones** | Las soluciones a los ejercicios son correctas, demostrando una implementaci√≥n adecuada de los conceptos y t√©cnicas requeridos. El estudiante muestra un dominio completo de los temas abordados. | Aunque las soluciones no son completamente correctas, se observa un entendimiento y aplicaci√≥n adecuada de los conceptos y t√©cnicas involucradas. Hay evidencia de esfuerzo y comprensi√≥n de los temas. | Las soluciones presentadas son en su mayor√≠a incorrectas. Se percibe un intento de resolver los ejercicios, pero hay una falta de comprensi√≥n de los conceptos y t√©cnicas esenciales. | No realiza la entrega |                       | **70**      |
| **Calidad de la entrega** | El notebook es claro y f√°cil de seguir, incluyendo comentarios detallados sobre el funcionamiento del c√≥digo en las celdas Markdown, lo que facilita la comprensi√≥n de las soluciones propuestas. | El notebook no es particularmente f√°cil de leer, pero a√∫n as√≠ incluye comentarios que explican el funcionamiento del c√≥digo en las celdas Markdown, mostrando un esfuerzo por aclarar la l√≥gica detr√°s del c√≥digo. | El notebook carece de comentarios acerca del funcionamiento del c√≥digo en las celdas Markdown, lo que dificulta la comprensi√≥n de las soluciones implementadas. | No realiza la entrega |                       | **20**      |
| **Tiempo de la entrega**  | La entrega se realiza a tiempo, cumpliendo con el plazo establecido para la presentaci√≥n de la actividad. | La entrega se realiza con una semana de atraso. Aunque fuera del plazo original, se considera adecuada para evaluar el trabajo presentado. | La entrega se realiza con m√°s de una semana de atraso, lo que indica un retraso significativo en la presentaci√≥n de la actividad. | No realiza la entrega |                       | **10**      |
|                           |                  |                       |                       |                       | **Ponderaci√≥n de la actividad** | **100 puntos** |

# Referencias

Aurelio AI. (s.f.). *LangChain Course*. Recuperado el 21 de mayo de 2025, de [https://www.aurelio.ai/course/langchain](https://www.aurelio.ai/course/langchain)

Chase, H., & Ng, A. (2023). *LangChain for LLM Application Development* [Curso en l√≠nea]. DeepLearning.AI. Disponible en [https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)


---
# Lecturas y material complementario

## üìö Lecturas recomendadas

### **T√≠tulo:** *How Trellix Uses LangChain to Enhance Cybersecurity*  
**Autor:** [LangChain]  
**Fecha de recuperaci√≥n:** 21 de mayo de 2025  
**URL:** [How Trellix Uses LangChain to Enhance Cybersecurity](https://blog.langchain.dev/customers-trellix/)
## üé• Videos recomendados

### **T√≠tulo:** *LangChain: Prompts, Parsers and Chaining | for Beginners*  
**Autor:** [Anub Gupta on Learn4Tarakki]  
**URL:** [LangChain: Prompts, Parsers and Chaining | for Beginners](https://www.youtube.com/watch?v=FHhJYxuIIA0)  
Este video ofrece una introducci√≥n amigable para principiantes sobre c√≥mo crear plantillas de prompts, utilizar parsers y encadenar componentes en LangChain.

### T√≠tulo: Interrupt 2025 Keynote | Harrison Chase | LangChain
**Autor:** [LangChain]
**URL:** Interrupt 2025 Keynote | Harrison Chase | LangChain

Este video presenta la keynote de Harrison Chase en la conferencia Interrupt 2025 de LangChain, donde se discute la evoluci√≥n de la ingenier√≠a de agentes y la visi√≥n de la compa√±√≠a para agentes inteligentes. Incluye reflexiones sobre la trayectoria de LangChain y anuncios de nuevas herramientas de desarrollo.




























